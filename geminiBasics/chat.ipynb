{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from apiKey import key\n",
    "genai.configure(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=genai.GenerativeModel('gemini-pro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini: I am a conversational AI or chatbot trained on a massive amount of text data. I am designed to be informative, comprehensive, and engaging. I can provide information and answer questions on a wide range of topics, including general knowledge, science, history, pop culture, and current events.\n",
      "\n",
      "I am also designed to be helpful and assist with various tasks, such as language translation, summarizing text, generating text, solving math problems, answering trivia questions, brainstorming ideas, and engaging in creative writing.\n",
      "\n",
      "Additionally, I am able to generate human-like text, translate languages, write various forms of creative content, and understand and respond to complex instructions and questions. However, it is important to note that I am still under development and may not always be able to provide accurate or comprehensive information, and I cannot access information created after my most recent training cutoff date, which is March 2022.\n",
      "Gemini: My token limit is 2048 tokens per response. This means that my responses are limited to 2048 tokens in length. A token is a basic unit of text, usually a word or punctuation mark. This limitation is in place to ensure that my responses are concise and relevant, and to prevent me from generating overly long or repetitive text.\n",
      "\n",
      "However, there are a few things that can affect the token length of my responses:\n",
      "\n",
      "* **The complexity of the question or prompt:** If the question or prompt is complex or requires a detailed explanation, my response may be longer and exceed the 2048 token limit.\n",
      "* **The amount of information available:** If there is a lot of information available on a topic, my response may be longer in order to provide a comprehensive answer.\n",
      "* **The style of the response:** If the response is written in a more formal or academic style, it may be longer than a response written in a more informal or conversational style.\n",
      "\n",
      "In general, I try to keep my responses as concise as possible while still providing all of the necessary information. If you find that my responses are too long or repetitive, please let me know and I will try to improve my output.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "content must not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     response\u001b[38;5;241m=\u001b[39m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGemini: \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\generativeai\\generative_models.py:358\u001b[0m, in \u001b[0;36mChatSession.send_message\u001b[1;34m(self, content, generation_config, safety_settings, stream, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;129m@string_utils\u001b[39m\u001b[38;5;241m.\u001b[39mset_doc(_SEND_MESSAGE_DOC)\n\u001b[0;32m    349\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_message\u001b[39m(\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    357\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m generation_types\u001b[38;5;241m.\u001b[39mGenerateContentResponse:\n\u001b[1;32m--> 358\u001b[0m     content \u001b[38;5;241m=\u001b[39m \u001b[43mcontent_types\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content\u001b[38;5;241m.\u001b[39mrole:\n\u001b[0;32m    360\u001b[0m         content\u001b[38;5;241m.\u001b[39mrole \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_USER_ROLE\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\google\\generativeai\\types\\content_types.py:194\u001b[0m, in \u001b[0;36mto_content\u001b[1;34m(content)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_content\u001b[39m(content: ContentType):\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m content:\n\u001b[1;32m--> 194\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent must not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content, Mapping):\n\u001b[0;32m    197\u001b[0m         content \u001b[38;5;241m=\u001b[39m _convert_dict(content)\n",
      "\u001b[1;31mValueError\u001b[0m: content must not be empty"
     ]
    }
   ],
   "source": [
    "chat=model.start_chat()\n",
    "while True:\n",
    "    message=input(\"You: \")\n",
    "    response=chat.send_message(message)\n",
    "    print(\"Gemini: \"+response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
